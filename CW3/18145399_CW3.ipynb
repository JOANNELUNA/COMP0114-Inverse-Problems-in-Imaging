{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP0114. Literature Study CW3\n",
    "#### Student ID: 18145399\n",
    "## Reconstruction with Sparsity Constraints\n",
    "### 1. Critical Analysis\n",
    "#### Influential\n",
    "Paper [1] by Candès, Romberg, and Tao, which was published in 2005, has had a significant impact on the field of compressed sensing and sparse signal recovery. The paper introduced the concept of using $\\ell_1$-regularization to recover sparse signals from incomplete and noisy measurements, which has become a standard technique used in various applications.\n",
    "\n",
    "The paper provides theoretical guarantees for the accuracy of the recovery method under certain conditions on the measurement matrix and the sparsity of the signal. It also demonstrates that the $\\ell_1$-regularization method can recover approximately sparse signals, which has important practical implications.\n",
    "\n",
    "As of March 2023, the paper has been cited 8,487 times according to Google Scholar, indicating its significance in the field. The paper has also led to numerous follow-up works that have further extended and refined the theory and applications of compressed sensing, resulting in high citation numbers for those works as well.\n",
    "\n",
    "Overall, the paper [1] has had a highly influential impact on research and practical applications in various fields, including computer science, electrical engineering, mathematics, and physics. Its continued citation and impact over the years further demonstrate its significance and lasting contribution to the field.\n",
    "\n",
    "The paper by J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, titled _\"Robust Face Recognition via Sparse Representation,\"_ is an example of a paper that cites the influential paper [1] by Candès, Romberg, and Tao. The paper builds on the framework introduced in [1] by applying the idea of sparse signal representation to the problem of face recognition in the presence of varying expressions, illumination, occlusion, and disguise.\n",
    "\n",
    "The paper has been cited 7,369 times (as of March 2023), indicating its significant impact in the field of computer vision and pattern recognition. The paper proposed a general classification algorithm for object recognition based on a sparse representation computed using $\\ell_1$-minimization. The proposed framework provided insights into feature extraction and robustness to occlusion, showing that unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces.\n",
    "\n",
    "Moreover, the paper proposed a method to handle errors due to occlusion and corruption by exploiting the sparsity of these errors with respect to the standard (pixel) basis. The theory of sparse representation helped predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion.\n",
    "\n",
    "Overall, the paper has had a significant impact on the field of face recognition and pattern recognition. Its high citation count and the extensive experiments conducted to validate the proposed algorithm and claims demonstrate its practical importance and impact. The paper is an example of how the ideas presented in [1] have been applied and extended to various domains, further highlighting the significance of the original paper in the field of compressed sensing and sparse signal recovery.\n",
    "\n",
    "\n",
    "#### Exactly reconstruction of signal from a discrete set of samples\n",
    "A signal can be exactly reconstructed from a discrete set of samples if the samples satisfy the Nyquist-Shannon sampling theorem, which states that the sampling rate must be at least twice the maximum frequency component in the signal. In other words, if the signal is bandlimited, meaning that its frequency content is limited to a certain range, then it can be exactly reconstructed from its samples.\n",
    "\n",
    "However, if the samples are incomplete or contaminated with noise, then exact reconstruction may not be possible. In this case, the conditions under which exact recovery is possible depend on the properties of the sampling matrix $A$ and the sparsity of the signal $x_0$. If $A$ satisfies certain conditions such as a uniform uncertainty principle and x0 is sufficiently sparse, then exact or nearly exact recovery is possible through the use of $l1$-regularization techniques. For example, if $A$ is a Gaussian random matrix, then stable recovery can occur for almost all such $A$'s provided that the number of nonzeros in $x_0$ is of about the same order as the number of observations. Similarly, if one observes only a few Fourier samples of $x_0$, stable recovery can occur for almost any set of n coefficients provided that the number of nonzeros is of the order of  $n/[\\log m]^6$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Numerical Experiments\n",
    "To reproduce the results in Part 3 of the paper, follow the steps outlined on page 8 of the paper:\n",
    "* First, generate a random n by m matrix $A$ (where $n <= m$). \n",
    "* Then create an original signal $x_0$ that is sparsely populated with non-zero entries. \n",
    "* Calculate $y = Ax_0 + e$ where $e$ represents noise terms added to our observations. \n",
    "* Finally, use the 1-regularization problem as described in the paper to recover $x_0$ from incomplete and contaminated observations $y$. This should result in stable recovery for almost all such matrices provided there are enough nonzeros present in $x_0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem\n",
    "$$ \\min ‖f‖_1   \\text{,    subject to  } ‖Af −y‖_2 ≤ \\epsilon, $$\n",
    "where $y$ is a $300 × 1$ vector $A$ is a $300 × 1024$ matrix and $f$ is a $1024 × 1$ vector, we can follow the method outlined in the paper as described above.\n",
    "\n",
    "The singular spectrum of the matrix $A$ used for reproducing is described as a designed singular value matrix $W$ with dimensions $1024 × 1024$, where the singular values decay exponentially with index. To construct the matrix $A$, two random orthogonal bases $U$ and $V$ are multiplied by $W$, such that $A = U * W * V^T$. Since $A$ is a $300 × 1024$ matrix, its singular values will be non-zero for the first $300$ indices and zero for the remaining $724$ indices. Therefore, original matrix $A$ has the singular spectrum of $exp(-k/10)$ where $k$ is $300$.\n",
    "\n",
    "#### Solve Basis Pursuit De-Noising problem with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmas: [0.01 0.02 0.05 0.1  0.2  0.5 ]\n",
      "epsilons: [0.18681268555846617, 0.37362537111693234, 0.9340634277923309, 1.8681268555846617, 3.7362537111693235, 9.340634277923309]\n",
      "errors: [1.043749633382624, 1.0031405633933308, 1.0000000000000004, 1.0000000000000002, 1.0000000000000002, 1.0000000000000002]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import cvxpy as cp\n",
    "\n",
    "# construct matrix A\n",
    "A = np.random.randn(300,1024)\n",
    "U = scipy.linalg.orth(A) #(300, 300)\n",
    "V = scipy.linalg.orth(np.transpose(A)) #(1024, 300)\n",
    "# singular values are decreasing exponentially with a decay rate of 10\n",
    "W = np.diag(np.exp(-np.arange(300)/10)) #(300, 300)\n",
    "A = np.dot(np.dot(U, W), V.T) #(300, 1024)\n",
    "\n",
    "# Set parameters\n",
    "m = 1024\n",
    "n = 300\n",
    "k = 50\n",
    "lam = 2\n",
    "sigmas = np.array([0.01, 0.02, 0.05, 0.1, 0.2, 0.5])\n",
    "num_trials = 10\n",
    "errors = []\n",
    "epsilons = []\n",
    "\n",
    "for sigma in sigmas:\n",
    "    error_sum = 0\n",
    "    for j in range(num_trials):\n",
    "        # Generate sparse 1D signals f0 with 50 nonzero components\n",
    "        f0 = np.zeros(m) # 1024\n",
    "        np.random.seed(0)\n",
    "        indices = np.random.choice(m, k, replace=False)\n",
    "        f0[indices] = np.random.choice([-1, 1], k)\n",
    "\n",
    "        # Generate error e with desired mean and standard deviation\n",
    "        mean_chi2 = sigma**2 * n  # mean of chi-square distribution\n",
    "        std_chi2 = sigma**2 * np.sqrt(2*n)  # standard deviation of chi-square distribution\n",
    "        e = np.random.randn(n) * sigma\n",
    "        e_norm2 = np.linalg.norm(e)**2\n",
    "        e *= np.sqrt(mean_chi2 / e_norm2)\n",
    "\n",
    "        # Normalize e to have desired mean and standard deviation of chi-square distribution\n",
    "        e_norm2 = np.linalg.norm(e)**2\n",
    "        e *= np.sqrt(mean_chi2 / e_norm2)\n",
    "        e *= std_chi2 / np.sqrt(e_norm2)\n",
    "\n",
    "        # Solve Basis Pursuit Denoising problem\n",
    "        epsilon = np.sqrt(sigma**2 * (n + lam*np.sqrt(2*n)))\n",
    "        \n",
    "        f = cp.Variable(m)\n",
    "        objective = cp.Minimize(cp.norm(f, 1))\n",
    "        constraints = [cp.norm(A @ f - A @ f0 - e, 2) <= epsilon]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve(solver=cp.ECOS)\n",
    "\n",
    "        # Compute error\n",
    "        try:\n",
    "            if f.value is not None:\n",
    "                error = np.linalg.norm(f0 - f.value, 2) / np.linalg.norm(f0, 2)\n",
    "                error_sum += error\n",
    "            else:\n",
    "                print(\"Optimization problem did not converge.\")\n",
    "        except cp.error.SolverError:\n",
    "            print(\"Solver error occurred. Please check the optimization problem formulation.\")\n",
    "        \n",
    "        \n",
    "    # Compute average error over all trials\n",
    "    epsilons.append(epsilon)\n",
    "    errors.append(error_sum / num_trials)\n",
    "\n",
    "# Print results\n",
    "print(\"sigmas:\", sigmas)\n",
    "print(\"epsilons:\", epsilons)\n",
    "print(\"errors:\", errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "There could be several reasons why my results are different eventhough I use the same lambda, num_trials, sigmas and epsilons as used in the paper:\n",
    "\n",
    "* The version of CVXPY used in this implementation might be different to the version used in the paper.\n",
    "* The method used to calculate the error in this implementation might differs from the method used in the referenced paper, possibly due to my misunderstanding of the procedure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve Basis Pursuit problem without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmas: [0.01 0.02 0.05 0.1  0.2  0.5 ]\n",
      "epsilons: [0.18681268555846617, 0.37362537111693234, 0.9340634277923309, 1.8681268555846617, 3.7362537111693235, 9.340634277923309]\n",
      "errors: [1.0440050429894006, 1.0140090079137192, 1.0000000000000013, 1.0000000000000004, 1.0000000000000002, 1.0000000000000002]\n"
     ]
    }
   ],
   "source": [
    "errors_bp = []\n",
    "epsilons_bp = []\n",
    "for sigma in sigmas:\n",
    "    error_sum = 0\n",
    "    for j in range(num_trials):\n",
    "        # Generate sparse 1D signals f0 with 50 nonzero components\n",
    "        f0 = np.zeros(m) # 1024\n",
    "        indices = np.random.choice(m, k, replace=False)\n",
    "        f0[indices] = np.random.choice([-1, 1], k)\n",
    "\n",
    "        # Solve Basis Pursuit Denoising problem\n",
    "        epsilon = np.sqrt(sigma**2 * (n + lam*np.sqrt(2*n)))\n",
    "        \n",
    "        f = cp.Variable(m)\n",
    "        objective = cp.Minimize(cp.norm(f, 1))\n",
    "        constraints = [cp.norm(A @ f - A @ f0, 2) <= epsilon]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve()\n",
    "\n",
    "        # Compute error\n",
    "        try:\n",
    "            if f.value is not None:\n",
    "                error = np.linalg.norm(f0 - f.value, 2) / np.linalg.norm(f0, 2)\n",
    "                error_sum += error\n",
    "            else:\n",
    "                print(\"Optimization problem did not converge.\")\n",
    "        except cp.error.SolverError:\n",
    "            print(\"Solver error occurred. Please check the optimization problem formulation.\")\n",
    "        \n",
    "        \n",
    "    # Compute average error over all trials\n",
    "    epsilons_bp.append(epsilon)\n",
    "    errors_bp.append(error_sum / num_trials)\n",
    "\n",
    "# Print results\n",
    "print(\"sigmas:\", sigmas)\n",
    "print(\"epsilons:\", epsilons_bp)\n",
    "print(\"errors:\", errors_bp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difeerent matrix\n",
    "Construct another matrix with the same singular vectors as $A$ above but where the singular spectrum is changed to $exp(-k/100)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmas: [0.01 0.02 0.05 0.1  0.2  0.5 ]\n",
      "epsilons: [0.18681268555846617, 0.37362537111693234, 0.9340634277923309, 1.8681268555846617, 3.7362537111693235, 9.340634277923309]\n",
      "errors: [0.36800827349775195, 0.6278064738862581, 0.9226299168425756, 1.0000000000000007, 1.0000000000000013, 1.0000000000000002]\n"
     ]
    }
   ],
   "source": [
    "# construct another matrix with the same singular vectors as A but with a different singular spectrum\n",
    "# singular values are decreasing exponentially with a decay rate of 100\n",
    "W2 = np.diag(np.exp(-np.arange(300)/100)) #(300, 300)\n",
    "A2 = np.dot(np.dot(U, W2), V.T) #(300, 1024)\n",
    "\n",
    "errors_A2 = []\n",
    "epsilons_A2 = []\n",
    "\n",
    "for sigma in sigmas:\n",
    "    error_sum = 0\n",
    "    for j in range(num_trials):\n",
    "        # Generate sparse 1D signals f0 with 50 nonzero components\n",
    "        f0 = np.zeros(m) # 1024\n",
    "        np.random.seed(0)\n",
    "        indices = np.random.choice(m, k, replace=False)\n",
    "        f0[indices] = np.random.choice([-1, 1], k)\n",
    "\n",
    "        # Generate error e with desired mean and standard deviation\n",
    "        mean_chi2 = sigma**2 * n  # mean of chi-square distribution\n",
    "        std_chi2 = sigma**2 * np.sqrt(2*n)  # standard deviation of chi-square distribution\n",
    "        e = np.random.randn(n) * sigma\n",
    "        e_norm2 = np.linalg.norm(e)**2\n",
    "        e *= np.sqrt(mean_chi2 / e_norm2)\n",
    "\n",
    "        # Normalize e to have desired mean and standard deviation of chi-square distribution\n",
    "        e_norm2 = np.linalg.norm(e)**2\n",
    "        e *= np.sqrt(mean_chi2 / e_norm2)\n",
    "        e *= std_chi2 / np.sqrt(e_norm2)\n",
    "\n",
    "        # Solve Basis Pursuit Denoising problem\n",
    "        epsilon = np.sqrt(sigma**2 * (n + lam*np.sqrt(2*n)))\n",
    "        \n",
    "        f = cp.Variable(m)\n",
    "        objective = cp.Minimize(cp.norm(f, 1))\n",
    "        constraints = [cp.norm(A2 @ f - A2 @ f0 - e, 2) <= epsilon]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve(solver=cp.ECOS)\n",
    "\n",
    "        # Compute error\n",
    "        try:\n",
    "            if f.value is not None:\n",
    "                error = np.linalg.norm(f0 - f.value, 2) / np.linalg.norm(f0, 2)\n",
    "                error_sum += error\n",
    "            else:\n",
    "                print(\"Optimization problem did not converge.\")\n",
    "        except cp.error.SolverError:\n",
    "            print(\"Solver error occurred. Please check the optimization problem formulation.\")\n",
    "        \n",
    "        \n",
    "    # Compute average error over all trials\n",
    "    epsilons_A2.append(epsilon)\n",
    "    errors_A2.append(error_sum / num_trials)\n",
    "\n",
    "# Print results\n",
    "print(\"sigmas:\", sigmas)\n",
    "print(\"epsilons:\", epsilons_A2)\n",
    "print(\"errors:\", errors_A2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "Changing the singular spectrum of $A$ to $exp(-k/100)$ may reduce the stability of the signal recovery process using the corresponding matrix $A2$ compared to the original matrix $A$ with the singular spectrum $exp(-k/10)$. This is because the rate of decay of the singular values in $A2$ is slower than in $A$ (i.e. the exponent in $A2$ is smaller than in $A$), which implies that the higher singular values will have a greater impact on the reconstruction of $f$ in $A2$. Therefore, the recovery of $f$ from $A2$ may be less precise and more susceptible to noise and errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
