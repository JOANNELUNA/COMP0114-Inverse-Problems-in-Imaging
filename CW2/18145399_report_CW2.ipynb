{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOMP0114 Inverse Problems in Imaging. Coursework 2\n",
    "### Student ID: 18145399\n",
    "## Week 1 \n",
    "### 1. Convolution and deconvolution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Read a gray colormap image from the given URL and convert it to a float, normalise and display it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![solution](1a.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  Write a function that takes in an image $f$ and outputs the blurred image $Af$ with convolution mapping.\n",
    "$$g = Af_{true} + n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage.filters as filters\n",
    "\n",
    "def apply_convolution(f, sigma, theta):\n",
    "    # Apply Gaussian filter\n",
    "    g = filters.gaussian_filter(f, sigma)\n",
    "    \n",
    "    # Add noise to blurred image\n",
    "    w, h = g.shape\n",
    "    noise = np.random.randn(w, h)\n",
    "    g = g + theta * noise\n",
    "    \n",
    "    return g  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Deconvolve using normal equations, i.e. find $f_α$ as the solution to\n",
    "$$(A^TA + αI)f_α = A^Tg$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as splinalg\n",
    "\n",
    "def ATA_operator(f, sigma, alpha):\n",
    "    # Apply A^T A + alpha*I operator to f\n",
    "    Af = filters.gaussian_filter(f, sigma) \n",
    "    ATAf = filters.gaussian_filter(Af, sigma)\n",
    "    return ATAf + alpha * f\n",
    "\n",
    "def deconvolve_normal_equations(g, sigma, alpha):\n",
    "    # Set up linear operator for ATA\n",
    "    M, N = g.shape\n",
    "    A = sparse.linalg.LinearOperator((M*N, M*N), matvec=lambda x: np.ravel(ATA_operator(x.reshape(g.shape), sigma, alpha)))\n",
    "\n",
    "    # Compute ATg\n",
    "    ATg = np.ravel(g)\n",
    "\n",
    "    # Solve linear system using GMRES\n",
    "    f_alpha, info = splinalg.gmres(A, ATg)\n",
    "\n",
    "    return f_alpha.reshape((M, N))\n",
    "\n",
    "def normal_info(g, sigma, alpha):\n",
    "    # Set up linear operator for ATA\n",
    "    M, N = g.shape\n",
    "    A = sparse.linalg.LinearOperator((M*N, M*N), matvec=lambda x: np.ravel(ATA_operator(x.reshape(g.shape), sigma, alpha)))\n",
    "\n",
    "    # Compute ATg\n",
    "    ATg = np.ravel(g)\n",
    "\n",
    "    # Solve linear system using GMRES\n",
    "    f_alpha, info = splinalg.gmres(A, ATg)\n",
    "\n",
    "    return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Deconvolve by solving the augmented equations.\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "A\\\\\n",
    "\\sqrt{\\alpha}I\n",
    "\\end{pmatrix}f \n",
    "= \n",
    "\\begin{pmatrix}\n",
    "g\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_f(f):\n",
    "    # Implementation of the augmented matrix multiplication\n",
    "    y = filters.gaussian_filter(f, sigma)\n",
    "    z = filters.gaussian_filter(y, sigma)\n",
    "    M_f = np.vstack([np.ravel(z), np.sqrt(alpha)*np.ravel(f)])\n",
    "    return M_f\n",
    "\n",
    "def MT_b(b):\n",
    "    # Implementation of the transposed augmented matrix multiplication\n",
    "    global g\n",
    "    M, N = g.shape\n",
    "    g_vec = b[:M*N]\n",
    "    f_vec = b[M*N:]\n",
    "    g = np.reshape(g_vec, (M, N))\n",
    "    y = filters.gaussian_filter(g, sigma)\n",
    "    z = filters.gaussian_filter(y, sigma)\n",
    "    MT_b = np.ravel(z) + np.sqrt(alpha)*np.ravel(f_vec)\n",
    "    return MT_b\n",
    "\n",
    "def solve_augmented_equations(g, sigma, alpha):\n",
    "    # Define linear operator for lsqr\n",
    "    M, N = g.shape\n",
    "    size = M*N\n",
    "    A = sparse.linalg.LinearOperator((2*size, size), matvec=M_f, rmatvec=MT_b)\n",
    "\n",
    "    # Concatenate g with a zero vector\n",
    "    b = np.vstack([np.reshape(g,(size,1)), np.zeros((size, 1))])\n",
    "    \n",
    "    # Solve linear system using lsqr\n",
    "    f_lsqr= splinalg.lsqr(A, b)[0]\n",
    "\n",
    "    return f_lsqr[:g.size].reshape(g.shape)\n",
    "\n",
    "def augmented_info(g, sigma, alpha):\n",
    "    # Define linear operator for lsqr\n",
    "    M, N = g.shape\n",
    "    size = M*N\n",
    "    A = sparse.linalg.LinearOperator((2*size, size), matvec=M_f, rmatvec=MT_b)\n",
    "\n",
    "    # Concatenate g with a zero vector\n",
    "    b = np.vstack([np.reshape(g,(size,1)), np.zeros((size, 1))])\n",
    "    \n",
    "    # Solve linear system using lsqr\n",
    "    info = splinalg.lsqr(A, b)[1]\n",
    "\n",
    "    return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance to the one used in c.), in terms of number of iterations required to achieve convergence:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method c: Converged in 0 iterations (0.03956246376037598 seconds) \n",
    "\n",
    "Method d: Converged in 2 iterations (0.7001798152923584 seconds)\n",
    "\n",
    "![solution](1e.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "#### The choice of the value of alpha\n",
    "When the value of the regularization parameter alpha was set to 0.01, the deblurred image produced by Method c was unclear. However, when alpha was set to 1, the deblurred image became much clearer.\n",
    "\n",
    "It is possible that alpha=0.01 was too small, and the regularization term had little effect in suppressing the noise in the observed image. As a result, the noise dominated the solution, leading to an unclear deblurred image. On the other hand, alpha=1 may have been a better choice, as it struck a good balance between noise suppression and image fidelity, resulting in a clearer deblurred image.\n",
    "\n",
    "#### The choice of Deconvolution method \n",
    "Method c is faster and requires fewer iterations to converge than Method d.\n",
    "\n",
    "Method c uses the normal equations, which can be solved using a Krylov solver such as PCG or GMRES. The normal equations can be derived from the optimality conditions for the Tikhonov regularization problem, and their solution provides the optimal solution to the deblurring problem. Since the normal equations involve a symmetric positive definite matrix, a Krylov solver can efficiently solve the linear system without requiring an explicit matrix representation of the convolution operator.\n",
    "\n",
    "On the other hand, Method d uses an augmented equation approach, which involves solving a linear least squares problem using a solver such as lsqr. This method may require more iterations to converge because the least squares problem may not have a closed-form solution and the iterative solver may need to compute many iterations to approximate the solution. In addition, the implementation of the augmented equation method requires more memory to store the augmented system matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2:\n",
    "### 2. Choose a regularisation parameter α\n",
    "i)  Discrepency Principle\n",
    "\n",
    "The Discrepancy Principle method was used to find the optimal values of α for the solutions obtained using both the normal equation and augmented equation methods. The optimal value of α was found to be `1e-5` for the normal equation method and `1e-2` for the augmented equation method. The initial guess for the value of α was set to `1e-5` for the normal equation method and `1e-2` for the augmented equation method, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root,brentq\n",
    "\n",
    "# method c\n",
    "\n",
    "def discrepancy_function_nor(alpha, g, sigma):\n",
    "    # Compute the residual\n",
    "    f_alpha = deconvolve_normal_equations(g, sigma, alpha)\n",
    "    r_alpha = apply_convolution(f_alpha, sigma, 0) - g\n",
    "    \n",
    "    r_norm = np.linalg.norm(r_alpha)\n",
    "\n",
    "    # Compute the discrepancy principle\n",
    "    n = g.size\n",
    "    sigma_sq = (r_norm / np.sqrt(n))**2\n",
    "    dp = 1/n * r_norm**2 - sigma_sq\n",
    "    \n",
    "    return dp\n",
    "\n",
    "def find_optimal_alpha_dp_nor(g, sigma):\n",
    "    # Set up alpha range\n",
    "    alpha_min = 1e-5\n",
    "    alpha_max = 1\n",
    "    \n",
    "    # Find the value of alpha that gives the zero of the DP(alpha) function\n",
    "    try:\n",
    "        # alpha_dp = brentq(discrepancy_function_nor, alpha_min, alpha_max, args=(g, sigma))\n",
    "        alpha_dp = root(discrepancy_function_nor, alpha_min, args=(g, sigma)).x[0]\n",
    "\n",
    "        print(\"Optimal alpha value using Discrepancy Principle for deconvolution using normal equations: \", alpha_dp)\n",
    "    except ValueError:\n",
    "        print(\"Failed to find optimal alpha using Discrepancy Principle.\")\n",
    "    return alpha_dp\n",
    "\n",
    "alpha_dp_c = find_optimal_alpha_dp_nor(g,sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) L-Curve\n",
    "\n",
    "The L-curve method was used to determine the optimal value of α for the normal equation method, and it was found to be `5.34e-1`. However, this method cannot be used for the augmented equation method.\n",
    "\n",
    "![solution](lcurve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range of alpha values to test\n",
    "alphas = np.logspace(-1, 1, num=100)\n",
    "\n",
    "# method c\n",
    "def l_curve(g, sigma, alphas, deconvolve_fun):\n",
    "    norm_f = []\n",
    "    norm_residual = []\n",
    "    \n",
    "    \n",
    "    for alpha in alphas:\n",
    "        if deconvolve_fun == 'normal equations':\n",
    "            # Compute f for current alpha\n",
    "            f_alpha = deconvolve_normal_equations(g, sigma, alpha)\n",
    "        elif deconvolve_fun == 'augmented equations':\n",
    "            print('L-curve is not applicable to augmented equations method')\n",
    "        # Compute residual and f norms\n",
    "        r_alpha = g - f_alpha #gaussian_filter(f_alpha, sigma)\n",
    "        norm_f.append(np.linalg.norm(f_alpha))\n",
    "        norm_residual.append(np.linalg.norm(r_alpha))\n",
    "        \n",
    "\n",
    "    return norm_f, norm_residual\n",
    "\n",
    "# Compute the L-curve\n",
    "norm_f_nor, norm_residual_nor = l_curve(g, sigma, alphas, 'normal equations')\n",
    "\n",
    "\n",
    "def plot_l_curve(norm_f, norm_residual, deconvolve_fun):\n",
    "    assert deconvolve_fun == 'normal equations', 'L-curve is not applicable to augmented equations method'\n",
    "    # Plot the L-curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.loglog(norm_residual, norm_f)\n",
    "    plt.xlabel('Norm of residual')\n",
    "    plt.ylabel('Norm of solution')\n",
    "    plt.title('L-Curve for solving {}'.format(deconvolve_fun))\n",
    "\n",
    "    # Find the optimal alpha value based on the L-curve\n",
    "    curvature = []\n",
    "    for i in range(1, len(alphas)-1):\n",
    "        dx1 = norm_residual[i] - norm_residual[i-1]\n",
    "        dy1 = norm_f[i] - norm_f[i-1]\n",
    "        dx2 = norm_residual[i+1] - norm_residual[i]\n",
    "        dy2 = norm_f[i+1] - norm_f[i]\n",
    "        curvature.append(abs(dx1*dy2 - dx2*dy1) / ((dx1**2 + dy1**2)**1.5 * (dx2**2 + dy2**2)**1.5))\n",
    "\n",
    "    optimal_index = np.argmax(curvature) + 1\n",
    "    alpha_l = alphas[optimal_index]\n",
    "\n",
    "    # Plot the optimal alpha value\n",
    "    plt.axvline(norm_residual[optimal_index], linestyle='--', color='gray')\n",
    "    plt.axhline(norm_f[optimal_index], linestyle='--', color='gray')\n",
    "    plt.text(norm_residual[optimal_index]*1.1, norm_f[optimal_index]*0.9, 'alpha_c = {:.2e}'.format(alpha_l))\n",
    "\n",
    "    plt.show()\n",
    "    return alpha_l\n",
    "\n",
    "alpha_l_c = plot_l_curve(norm_f_nor, norm_residual_nor, 'normal equations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "#### Difference in value obtained \n",
    "\n",
    "It's possible that the optimal alpha found by the L-curve method is different from the optimal alpha found by the Discrepancy Principle method because they use different criteria to determine the optimal alpha.\n",
    "\n",
    "The L-curve method tries to find the point on the L-curve that balances the trade-off between the residual norm and the regularization norm. This point represents the optimal alpha value that provides a good balance between overfitting and underfitting.\n",
    "\n",
    "On the other hand, the Discrepancy Principle method tries to find the alpha value that provides a solution that is consistent with the noise level in the data. The optimal alpha value is the one that satisfies the principle that the residual norm is approximately equal to the noise level.\n",
    "\n",
    "It's possible that the optimal alpha values found by these two methods are different because they are optimizing for different criteria. The L-curve method is optimizing for a trade-off between the residual and regularization norms, while the Discrepancy Principle method is optimizing for a solution that is consistent with the noise level.\n",
    "\n",
    "\n",
    "#### Results using these optimal alpha values\n",
    "##### 1) Results using DP optimal alpha value for normal equation\n",
    "Method c: Converged in 0 iterations (114.29729723930359 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.046361446380615234 seconds)\n",
    "\n",
    "![solution](result_dp_c.png)\n",
    "\n",
    "##### 2) Results using DP optimal alpha value for augmented equation\n",
    "Method c: Converged in 0 iterations (0.27139997482299805 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.04271340370178223 seconds)\n",
    "\n",
    "![solution](result_dp_d.png)\n",
    "\n",
    "##### 3) Results using L-curve optimal alpha value for normal equation\n",
    "Method c: Converged in 0 iterations (0.05376148223876953 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.04639315605163574 seconds)\n",
    "\n",
    "![solution](result_l_c.png)\n",
    "\n",
    "Based on the fact that the optimal values of alpha found using the Discrepancy Principle method are the same as initial guess alphas, as well as the fact that the result of the deblurred plots are unclear when using the optimal alphas found by the DP method, it shows that Discrepancy Principle method is not a good choice to find the optimal value of α for the data being used.\n",
    "\n",
    "The Discrepancy Principle method can be useful for finding an approximate range of α values that may be optimal for the given data, but it may not always be able to pinpoint the exact optimal value. In my case, the optimal value of α may lie outside of the range searched by the DP method, or may require a more fine-tuned search to find.\n",
    "\n",
    "Therefore, it may be necessary to explore other methods, such as the L-curve method or other regularization methods, to find the optimal value of α for the given data. Additionally, it may be helpful to visually inspect the deblurred plots to determine if the result is acceptable or if further optimization is needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using a regularisation term based on the spatial derivative\n",
    "#### a) Gradient operator $D$\n",
    "Construct the gradient operator and implement it as a function like the forward convolution.\n",
    "$$D = \\begin{pmatrix}\n",
    "\\nabla_x\\\\\n",
    "\\nabla_y\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_operator(f):\n",
    "    grad_x = np.diff(f, axis=1)\n",
    "    grad_y = np.diff(f, axis=0)\n",
    "    grad_x = np.pad(grad_x, ((0, 0), (0, 1)), mode='constant')\n",
    "    grad_y = np.pad(grad_y, ((0, 1), (0, 0)), mode='constant')\n",
    "    return np.stack([grad_x, grad_y], axis=2)\n",
    "\n",
    "grad_im = gradient_operator(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Normal and augmented equation solver\n",
    "Solve the gradient regularised problem with both normal and augmented equation solvers.\n",
    "\n",
    "In Question 1, we solve the regularised least square problem\n",
    "\n",
    "$$f_\\alpha = \\argmin_f||Af − g||_2^2 + α||f||_2^2$$\n",
    "\n",
    "where $||f||_2$ is chosen to be the regulariser. In this Question, we are going to use a new regulariser $||Df||_2$ to penalise the gradient instead. So the new problem will be \n",
    "\n",
    "$$f_\\alpha = \\argmin_f||Af − g||_2^2 + α||Df||_2^2$$\n",
    "\n",
    "Derive the normal equation for the gradient regularized problem to solve:\n",
    "\n",
    "$$(A^T A + \\alpha D^T D) f = A^T g$$\n",
    "\n",
    "Expanding the first term:\n",
    "\n",
    "$$(A^T A + \\alpha D^T D) f = f^T A^T A f + \\alpha f^T D^T D f$$\n",
    "\n",
    "Taking the derivative with respect to $x$ and setting it to zero:\n",
    "\n",
    "$$\\nabla_x(f^T A^T A f + \\alpha f^T D^T D f) = 2A^T Af + 2\\alpha D^T D f = 2A^T g$$\n",
    "\n",
    "This is the normal equation for the gradient regularized problem. It can be solved using the same method as before, but with the modified matrix:\n",
    "\n",
    "$$L = A^T A + \\alpha D^T D$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Chose a value for α, explain your choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3\n",
    "### 4. Construct an anisotropic derivative filter\n",
    "\n",
    "Rather than using the isotropic regulariser on the gradient, we can add weights and use an\n",
    "anisotropic regulariser $||\\sqrt{\\gamma|}Df||_2^2$. With the anisotropic derivative filter, we turn to solve\n",
    "$$fα = \\argmin_f||Af − g||_2^2 + α||\\sqrt{\\gamma}Df||_2^2$$\n",
    "Here $γ$ is termed the diffusivity. You should make $γ$ a diagonal matrix with values between 0\n",
    "and 1. Places where $γ = 0$ will not be smoothed by the regularisation term. You would ideally\n",
    "set γ based on the values of the edges in ftrue, but since this is not known (it is what you are\n",
    "trying to find!) you should use the edges in the data. Note that after defining $γ$ it is fixed for\n",
    "the optimisation procedure) we will consider varying it during optimisation.\n",
    "An example diffusivity is the Perona-Malik function\n",
    "$$γ(f) = exp(−|Df|/T) = exp(−\\sqrt{(∇_xf)^2 + (∇_yf)^2/T})$$\n",
    "for some threshold $T$ based on the maximum expected edge values in the image; this can be\n",
    "estimated from the norm of the image gradient. Note that $\\sqrt{·}$ here is an element-wise operation\n",
    "on the diagonal matrix $γ$, so to calculate $\\sqrt{\\gamma}D$ we use\n",
    "$$\\sqrt{\\gamma}D := \\begin{pmatrix}\n",
    "\\sqrt{\\gamma}∇_x\\\\\n",
    "\\sqrt{\\gamma}∇_y\n",
    "\\end{pmatrix}$$\n",
    "Note : The quantities $γ$, $|Df|$ , $∇_xf$, $∇_yf$ are all defined at each pixel. It may help to display them as images to aid your understanding of their meaning. The square operation on vectors $∇_xf$ and $∇_yf$ are also element-wise, as well as the square root operation and exponential operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
