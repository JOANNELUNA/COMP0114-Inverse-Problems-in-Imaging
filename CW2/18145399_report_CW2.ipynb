{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOMP0114 Inverse Problems in Imaging. Coursework 2\n",
    "### Student ID: 18145399\n",
    "## Week 1 \n",
    "### 1. Convolution and deconvolution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Read a gray colormap image from the given URL and convert it to a float, normalise and display it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![solution](1a.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  Write a function that takes in an image $f$ and outputs the blurred image $Af$ with convolution mapping.\n",
    "$$g = Af_{true} + n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage.filters as filters\n",
    "\n",
    "def apply_convolution(f, sigma, theta):\n",
    "    # Apply Gaussian filter\n",
    "    g = filters.gaussian_filter(f, sigma)\n",
    "    \n",
    "    # Add noise to blurred image\n",
    "    w, h = g.shape\n",
    "    noise = np.random.randn(w, h)\n",
    "    g = g + theta * noise\n",
    "    \n",
    "    return g  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Deconvolve using normal equations, i.e. find $f_α$ as the solution to\n",
    "$$(A^TA + αI)f_α = A^Tg$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as splinalg\n",
    "\n",
    "def ATA_operator(f, sigma, alpha):\n",
    "    # Apply A^T A + alpha*I operator to f\n",
    "    Af = filters.gaussian_filter(f, sigma) \n",
    "    ATAf = filters.gaussian_filter(Af, sigma)\n",
    "    return ATAf + alpha * f\n",
    "\n",
    "def deconvolve_normal_equations(g, sigma, alpha):\n",
    "    # Set up linear operator for ATA\n",
    "    M, N = g.shape\n",
    "    A = sparse.linalg.LinearOperator((M*N, M*N), matvec=lambda x: np.ravel(ATA_operator(x.reshape(g.shape), sigma, alpha)))\n",
    "\n",
    "    # Compute ATg\n",
    "    ATg = np.ravel(g)\n",
    "\n",
    "    # Solve linear system using GMRES\n",
    "    f_alpha, info = splinalg.gmres(A, ATg)\n",
    "\n",
    "    return f_alpha.reshape((M, N))\n",
    "\n",
    "def normal_info(g, sigma, alpha):\n",
    "    # Set up linear operator for ATA\n",
    "    M, N = g.shape\n",
    "    A = sparse.linalg.LinearOperator((M*N, M*N), matvec=lambda x: np.ravel(ATA_operator(x.reshape(g.shape), sigma, alpha)))\n",
    "\n",
    "    # Compute ATg\n",
    "    ATg = np.ravel(g)\n",
    "\n",
    "    # Solve linear system using GMRES\n",
    "    f_alpha, info = splinalg.gmres(A, ATg)\n",
    "\n",
    "    return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Deconvolve by solving the augmented equations.\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "A\\\\\n",
    "\\sqrt{\\alpha}I\n",
    "\\end{pmatrix}f \n",
    "= \n",
    "\\begin{pmatrix}\n",
    "g\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_f(f):\n",
    "    # Implementation of the augmented matrix multiplication\n",
    "    y = filters.gaussian_filter(f, sigma)\n",
    "    z = filters.gaussian_filter(y, sigma)\n",
    "    M_f = np.vstack([np.ravel(z), np.sqrt(alpha)*np.ravel(f)])\n",
    "    return M_f\n",
    "\n",
    "def MT_b(b):\n",
    "    # Implementation of the transposed augmented matrix multiplication\n",
    "    global g\n",
    "    M, N = g.shape\n",
    "    g_vec = b[:M*N]\n",
    "    f_vec = b[M*N:]\n",
    "    g = np.reshape(g_vec, (M, N))\n",
    "    y = filters.gaussian_filter(g, sigma)\n",
    "    z = filters.gaussian_filter(y, sigma)\n",
    "    MT_b = np.ravel(z) + np.sqrt(alpha)*np.ravel(f_vec)\n",
    "    return MT_b\n",
    "\n",
    "def solve_augmented_equations(g, sigma, alpha):\n",
    "    # Define linear operator for lsqr\n",
    "    M, N = g.shape\n",
    "    size = M*N\n",
    "    A = sparse.linalg.LinearOperator((2*size, size), matvec=M_f, rmatvec=MT_b)\n",
    "\n",
    "    # Concatenate g with a zero vector\n",
    "    b = np.vstack([np.reshape(g,(size,1)), np.zeros((size, 1))])\n",
    "    \n",
    "    # Solve linear system using lsqr\n",
    "    f_lsqr= splinalg.lsqr(A, b)[0]\n",
    "\n",
    "    return f_lsqr[:g.size].reshape(g.shape)\n",
    "\n",
    "def augmented_info(g, sigma, alpha):\n",
    "    # Define linear operator for lsqr\n",
    "    M, N = g.shape\n",
    "    size = M*N\n",
    "    A = sparse.linalg.LinearOperator((2*size, size), matvec=M_f, rmatvec=MT_b)\n",
    "\n",
    "    # Concatenate g with a zero vector\n",
    "    b = np.vstack([np.reshape(g,(size,1)), np.zeros((size, 1))])\n",
    "    \n",
    "    # Solve linear system using lsqr\n",
    "    info = splinalg.lsqr(A, b)[1]\n",
    "\n",
    "    return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance to the one used in c.), in terms of number of iterations required to achieve convergence:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method c: Converged in 0 iterations (0.03956246376037598 seconds) \n",
    "\n",
    "Method d: Converged in 2 iterations (0.7001798152923584 seconds)\n",
    "\n",
    "![solution](1e.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "#### The choice of the value of alpha\n",
    "When the value of the regularization parameter alpha was set to 0.01, the deblurred image produced by Method c was unclear. However, when alpha was set to 1, the deblurred image became much clearer.\n",
    "\n",
    "It is possible that alpha=0.01 was too small, and the regularization term had little effect in suppressing the noise in the observed image. As a result, the noise dominated the solution, leading to an unclear deblurred image. On the other hand, alpha=1 may have been a better choice, as it struck a good balance between noise suppression and image fidelity, resulting in a clearer deblurred image.\n",
    "\n",
    "#### The choice of Deconvolution method \n",
    "Method c is faster and requires fewer iterations to converge than Method d.\n",
    "\n",
    "Method c uses the normal equations, which can be solved using a Krylov solver such as PCG or GMRES. The normal equations can be derived from the optimality conditions for the Tikhonov regularization problem, and their solution provides the optimal solution to the deblurring problem. Since the normal equations involve a symmetric positive definite matrix, a Krylov solver can efficiently solve the linear system without requiring an explicit matrix representation of the convolution operator.\n",
    "\n",
    "On the other hand, Method d uses an augmented equation approach, which involves solving a linear least squares problem using a solver such as lsqr. This method may require more iterations to converge because the least squares problem may not have a closed-form solution and the iterative solver may need to compute many iterations to approximate the solution. In addition, the implementation of the augmented equation method requires more memory to store the augmented system matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2:\n",
    "### 2. Choose a regularisation parameter α\n",
    "i)  Discrepency Principle\n",
    "\n",
    "The Discrepancy Principle method was used to find the optimal values of α for the solutions obtained using both the normal equation and augmented equation methods. The optimal value of α was found to be `1e-5` for the normal equation method and `1e-2` for the augmented equation method. The initial guess for the value of α was set to `1e-5` for the normal equation method and `1e-2` for the augmented equation method, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root,brentq\n",
    "\n",
    "# method c\n",
    "\n",
    "def discrepancy_function_nor(alpha, g, sigma):\n",
    "    # Compute the residual\n",
    "    f_alpha = deconvolve_normal_equations(g, sigma, alpha)\n",
    "    r_alpha = apply_convolution(f_alpha, sigma, 0) - g\n",
    "    \n",
    "    r_norm = np.linalg.norm(r_alpha)\n",
    "\n",
    "    # Compute the discrepancy principle\n",
    "    n = g.size\n",
    "    sigma_sq = (r_norm / np.sqrt(n))**2\n",
    "    dp = 1/n * r_norm**2 - sigma_sq\n",
    "    \n",
    "    return dp\n",
    "\n",
    "def find_optimal_alpha_dp_nor(g, sigma):\n",
    "    # Set up alpha range\n",
    "    alpha_min = 1e-5\n",
    "    alpha_max = 1\n",
    "    \n",
    "    # Find the value of alpha that gives the zero of the DP(alpha) function\n",
    "    try:\n",
    "        # alpha_dp = brentq(discrepancy_function_nor, alpha_min, alpha_max, args=(g, sigma))\n",
    "        alpha_dp = root(discrepancy_function_nor, alpha_min, args=(g, sigma)).x[0]\n",
    "\n",
    "        print(\"Optimal alpha value using Discrepancy Principle for deconvolution using normal equations: \", alpha_dp)\n",
    "    except ValueError:\n",
    "        print(\"Failed to find optimal alpha using Discrepancy Principle.\")\n",
    "    return alpha_dp\n",
    "\n",
    "alpha_dp_c = find_optimal_alpha_dp_nor(g,sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) L-Curve\n",
    "\n",
    "The L-curve method was used to determine the optimal value of α for the normal equation method, and it was found to be `5.34e-1`. However, this method cannot be used for the augmented equation method.\n",
    "\n",
    "![solution](lcurve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range of alpha values to test\n",
    "alphas = np.logspace(-1, 1, num=100)\n",
    "\n",
    "# method c\n",
    "def l_curve(g, sigma, alphas, deconvolve_fun):\n",
    "    norm_f = []\n",
    "    norm_residual = []\n",
    "    \n",
    "    \n",
    "    for alpha in alphas:\n",
    "        if deconvolve_fun == 'normal equations':\n",
    "            # Compute f for current alpha\n",
    "            f_alpha = deconvolve_normal_equations(g, sigma, alpha)\n",
    "        elif deconvolve_fun == 'augmented equations':\n",
    "            print('L-curve is not applicable to augmented equations method')\n",
    "        # Compute residual and f norms\n",
    "        r_alpha = g - f_alpha #gaussian_filter(f_alpha, sigma)\n",
    "        norm_f.append(np.linalg.norm(f_alpha))\n",
    "        norm_residual.append(np.linalg.norm(r_alpha))\n",
    "        \n",
    "\n",
    "    return norm_f, norm_residual\n",
    "\n",
    "# Compute the L-curve\n",
    "norm_f_nor, norm_residual_nor = l_curve(g, sigma, alphas, 'normal equations')\n",
    "\n",
    "\n",
    "def plot_l_curve(norm_f, norm_residual, deconvolve_fun):\n",
    "    assert deconvolve_fun == 'normal equations', 'L-curve is not applicable to augmented equations method'\n",
    "    # Plot the L-curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.loglog(norm_residual, norm_f)\n",
    "    plt.xlabel('Norm of residual')\n",
    "    plt.ylabel('Norm of solution')\n",
    "    plt.title('L-Curve for solving {}'.format(deconvolve_fun))\n",
    "\n",
    "    # Find the optimal alpha value based on the L-curve\n",
    "    curvature = []\n",
    "    for i in range(1, len(alphas)-1):\n",
    "        dx1 = norm_residual[i] - norm_residual[i-1]\n",
    "        dy1 = norm_f[i] - norm_f[i-1]\n",
    "        dx2 = norm_residual[i+1] - norm_residual[i]\n",
    "        dy2 = norm_f[i+1] - norm_f[i]\n",
    "        curvature.append(abs(dx1*dy2 - dx2*dy1) / ((dx1**2 + dy1**2)**1.5 * (dx2**2 + dy2**2)**1.5))\n",
    "\n",
    "    optimal_index = np.argmax(curvature) + 1\n",
    "    alpha_l = alphas[optimal_index]\n",
    "\n",
    "    # Plot the optimal alpha value\n",
    "    plt.axvline(norm_residual[optimal_index], linestyle='--', color='gray')\n",
    "    plt.axhline(norm_f[optimal_index], linestyle='--', color='gray')\n",
    "    plt.text(norm_residual[optimal_index]*1.1, norm_f[optimal_index]*0.9, 'alpha_c = {:.2e}'.format(alpha_l))\n",
    "\n",
    "    plt.show()\n",
    "    return alpha_l\n",
    "\n",
    "alpha_l_c = plot_l_curve(norm_f_nor, norm_residual_nor, 'normal equations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "#### Difference in value obtained \n",
    "\n",
    "It's possible that the optimal alpha found by the L-curve method is different from the optimal alpha found by the Discrepancy Principle method because they use different criteria to determine the optimal alpha.\n",
    "\n",
    "The L-curve method tries to find the point on the L-curve that balances the trade-off between the residual norm and the regularization norm. This point represents the optimal alpha value that provides a good balance between overfitting and underfitting.\n",
    "\n",
    "On the other hand, the Discrepancy Principle method tries to find the alpha value that provides a solution that is consistent with the noise level in the data. The optimal alpha value is the one that satisfies the principle that the residual norm is approximately equal to the noise level.\n",
    "\n",
    "It's possible that the optimal alpha values found by these two methods are different because they are optimizing for different criteria. The L-curve method is optimizing for a trade-off between the residual and regularization norms, while the Discrepancy Principle method is optimizing for a solution that is consistent with the noise level.\n",
    "\n",
    "\n",
    "#### Results using these optimal alpha values\n",
    "##### 1) Results using DP optimal alpha value for normal equation\n",
    "Method c: Converged in 0 iterations (114.29729723930359 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.046361446380615234 seconds)\n",
    "\n",
    "![solution](result_dp_c.png)\n",
    "\n",
    "##### 2) Results using DP optimal alpha value for augmented equation\n",
    "Method c: Converged in 0 iterations (0.27139997482299805 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.04271340370178223 seconds)\n",
    "\n",
    "![solution](result_dp_d.png)\n",
    "\n",
    "##### 3) Results using L-curve optimal alpha value for normal equation\n",
    "Method c: Converged in 0 iterations (0.05376148223876953 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.04639315605163574 seconds)\n",
    "\n",
    "![solution](result_l_c.png)\n",
    "\n",
    "Based on the fact that the optimal values of alpha found using the Discrepancy Principle method are the same as initial guess alphas, as well as the fact that the result of the deblurred plots are unclear when using the optimal alphas found by the DP method, it shows that Discrepancy Principle method is not a good choice to find the optimal value of α for the data being used.\n",
    "\n",
    "The Discrepancy Principle method can be useful for finding an approximate range of α values that may be optimal for the given data, but it may not always be able to pinpoint the exact optimal value. In my case, the optimal value of α may lie outside of the range searched by the DP method, or may require a more fine-tuned search to find.\n",
    "\n",
    "Therefore, it may be necessary to explore other methods, such as the L-curve method or other regularization methods, to find the optimal value of α for the given data. Additionally, it may be helpful to visually inspect the deblurred plots to determine if the result is acceptable or if further optimization is needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using a regularisation term based on the spatial derivative\n",
    "#### a) Gradient operator $D$\n",
    "Construct the gradient operator and implement it as a function like the forward convolution.\n",
    "$$D = \\begin{pmatrix}\n",
    "\\nabla_x\\\\\n",
    "\\nabla_y\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_operator(f):\n",
    "    grad_x = np.diff(f, axis=1)\n",
    "    grad_y = np.diff(f, axis=0)\n",
    "    grad_x = np.pad(grad_x, ((0, 0), (0, 1)), mode='constant')\n",
    "    grad_y = np.pad(grad_y, ((0, 1), (0, 0)), mode='constant')\n",
    "    return np.stack([grad_x, grad_y], axis=2)\n",
    "\n",
    "grad_im = gradient_operator(im)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Normal equation solver\n",
    "Solve the gradient regularised problem with both normal and augmented equation solvers.\n",
    "\n",
    "In Question 1, we solve the regularised least square problem\n",
    "\n",
    "$$f_\\alpha = \\argmin_f||Af − g||_2^2 + α||f||_2^2$$\n",
    "\n",
    "where $||f||_2$ is chosen to be the regulariser. In this Question, we are going to use a new regulariser $||Df||_2$ to penalise the gradient instead. We want to solve the regularized least squares problem:\n",
    "\n",
    "$$f_\\alpha = \\arg\\min_f ||Af - g||_2^2 + \\alpha ||Df||_2^2,$$\n",
    "\n",
    "where $A$ is the data matrix, $g$ is the observation vector, $D$ is the gradient operator, and $\\alpha$ is the regularization parameter. To derive the normal equations, we first expand the second term as follows:\n",
    "\n",
    "$$\\alpha ||Df||_2^2 = \\alpha f^T D^T D f = f^T \\alpha D^T D f.$$\n",
    "\n",
    "Then, we form the Lagrangian:\n",
    "\n",
    "$$L(f, \\lambda) = ||Af - g||_2^2 + f^T \\alpha D^T D f + \\lambda^T (f - x),$$\n",
    "\n",
    "where $\\lambda$ is the Lagrange multiplier and $x$ is the solution to the constrained problem $f \\in {x | Dx = 0}$. Taking the derivative of $L$ with respect to $f$ and setting it to zero, we get:\n",
    "\n",
    "$$2A^T(Af - g) + 2\\alpha D^T D f + \\lambda = 0.$$\n",
    "\n",
    "Multiplying both sides by $D$ and using the fact that $D^T D = -\\nabla^2$, we get:\n",
    "\n",
    "$$-2\\alpha \\nabla^2 f + D\\lambda = 0.$$\n",
    "\n",
    "Since $D$ is a sparse matrix, we can solve for $\\lambda$ using a sparse linear system solver. Substituting this back into the previous equation, we get:\n",
    "\n",
    "$$(A^T A + \\alpha D^T D) f = A^T g.$$\n",
    "\n",
    "This is the normal equation for the gradient regularized problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Augmented equation solver\n",
    "To use the augmented Lagrangian method to solve the regularized least squares problem with gradient regularization, we can introduce a Lagrange multiplier $\\lambda$ and rewrite the problem as:\n",
    "\n",
    "$$\\min_f \\frac{1}{2}||Af - g||_2^2 + \\frac{\\alpha}{2}||Df||_2^2 + \\lambda^T(Df - y) + \\frac{\\mu}{2}||Df - y||_2^2,$$\n",
    "\n",
    "where $y$ is an intermediate variable and $\\mu$ is a penalty parameter that controls the degree of constraint violation. We can then apply the alternating direction method of multipliers (ADMM) to solve this problem iteratively.\n",
    "\n",
    "At each iteration, we update $f$ by solving the following least squares problem:\n",
    "\n",
    "$$(A^T A + \\alpha D^T D + \\mu I) f = A^T g + D^T (y - \\lambda/\\mu),$$\n",
    "\n",
    "where $I$ is the identity matrix. We can use any linear system solver to solve this equation, including LSQR.\n",
    "\n",
    "We then update $y$ by projecting $Df + \\lambda/\\mu$ onto the constraint set $Df = y$:\n",
    "\n",
    "$$y = \\max(0, |Df + \\lambda/\\mu| - \\alpha/\\mu) \\cdot \\operatorname{sign}(Df + \\lambda/\\mu).$$\n",
    "\n",
    "Finally, we update the Lagrange multiplier $\\lambda$ by:\n",
    "\n",
    "$$\\lambda = \\lambda + \\mu (Df - y).$$\n",
    "\n",
    "We repeat these steps until convergence. The penalty parameter $\\mu$ can be increased at each iteration to improve the convergence rate. However, if $\\mu$ is chosen too large, the method may become unstable. Choosing an appropriate value for $\\mu$ can be tricky and often requires some trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Chose a value for α, explain your choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3\n",
    "### 4. Anisotropic derivative filter\n",
    "\n",
    "To perform image denoising with an anisotropic derivative filter, we add weights to the gradient term to obtain the following regularized least squares problem:\n",
    "\n",
    "$$f_\\alpha = \\arg\\min_f ||Af - g||_2^2 + \\alpha ||\\sqrt{\\gamma}Df||_2^2,$$\n",
    "\n",
    "where $A$ is the data matrix, $g$ is the observation vector, $D$ is the gradient operator, $\\gamma$ is the diffusivity matrix, and $\\alpha$ is the regularization parameter. We construct $\\gamma$ as a diagonal matrix with entries between 0 and 1, where 0 indicates regions of the image where we do not want to smooth. One example diffusivity function is the Perona-Malik function:\n",
    "\n",
    "$$\\gamma(f) = \\exp(-|Df|/T) = \\exp(-\\sqrt{(\\partial_x f)^2 + (\\partial_y f)^2/T}),$$\n",
    "\n",
    "where $T$ is a threshold based on the maximum expected edge values in the image, which can be estimated from the norm of the image gradient. Note that $\\sqrt{\\cdot}$ here is an element-wise operation on the diagonal matrix $\\gamma$, so to calculate $\\sqrt{\\gamma}D$, we use\n",
    "\n",
    "$$\\sqrt{\\gamma}D = \\begin{pmatrix}\\sqrt{\\gamma}\\partial_x \\ \\sqrt{\\gamma}\\partial_y\\end{pmatrix}.$$\n",
    "\n",
    "To solve the regularized least squares problem, we can use the normal equation:\n",
    "\n",
    "$$(A^T A + \\alpha D^T \\gamma D) f = A^T g.$$\n",
    "\n",
    "We can solve this equation using any linear system solver, including LSQR.\n",
    "\n",
    "Alternatively, we can use the augmented Lagrangian method to solve the problem iteratively. At each iteration, we update $f$ by solving the following least squares problem:\n",
    "\n",
    "$$(A^T A + \\alpha D^T \\gamma D + \\mu I) f = A^T g + D^T \\gamma \\sqrt{\\gamma}(y - \\lambda/\\mu),$$\n",
    "\n",
    "where $y$ is an intermediate variable, $\\mu$ is a penalty parameter, and $\\lambda$ is the Lagrange multiplier. We can use any linear system solver to solve this equation, including LSQR.\n",
    "\n",
    "We then update $y$ by projecting $\\sqrt{\\gamma}(Df + \\lambda/\\mu)$ onto the constraint set $Df = y$:\n",
    "\n",
    "$$y = \\max(0, |\\sqrt{\\gamma}(Df + \\lambda/\\mu)| - \\alpha/\\mu) \\cdot \\operatorname{sign}(\\sqrt{\\gamma}(Df + \\lambda/\\mu)).$$\n",
    "\n",
    "Finally, we update the Lagrange multiplier $\\lambda$ by:\n",
    "\n",
    "$$\\lambda = \\lambda + \\mu \\sqrt{\\gamma}(Df - y).$$\n",
    "\n",
    "We repeat these steps until convergence. The penalty parameter $\\mu$ can be increased at each iteration to improve the convergence rate. However, if $\\mu$ is chosen too large, the method may become unstable. Choosing an appropriate value for $\\mu$ can be tricky and often requires some trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anisotropic_derivative_filter(g, alpha, T, max_iter=100, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Solve an optimization problem with an anisotropic derivative filter using the Perona-Malik diffusivity function.\n",
    "    \n",
    "    Args:\n",
    "        g (ndarray): the input image\n",
    "        alpha (float): the regularization strength\n",
    "        T (float): the threshold for the Perona-Malik function\n",
    "        max_iter (int): the maximum number of iterations (default: 100)\n",
    "        eps (float): the stopping criterion (default: 1e-5)\n",
    "        \n",
    "    Returns:\n",
    "        f (ndarray): the denoised image\n",
    "    \"\"\"\n",
    "    # Construct the gradient operator\n",
    "    Dx = spdiags([-1*np.ones(g.shape[0]), np.ones(g.shape[0])], [0, 1], g.shape[0]-1, g.shape[0])\n",
    "    Dx = Dx.todense()\n",
    "    Dx[-1, -1] = -1\n",
    "    Dx = np.kron(np.eye(g.shape[1]), Dx)\n",
    "    \n",
    "    Dy = spdiags([-1*np.ones(g.shape[1]), np.ones(g.shape[1])], [0, 1], g.shape[1]-1, g.shape[1])\n",
    "    Dy = Dy.todense()\n",
    "    Dy[-1, -1] = -1\n",
    "    Dy = np.kron(Dy, np.eye(g.shape[0]))\n",
    "    \n",
    "    D = np.vstack((Dx, Dy))\n",
    "    \n",
    "    # Initialize gamma\n",
    "    gradient_norm = np.sqrt(Dx @ g.flatten()**2 + Dy @ g.flatten()**2)\n",
    "    gamma = np.exp(-gradient_norm / T)\n",
    "    gamma = spdiags(gamma, 0, len(gradient_norm), len(gradient_norm)).todense()\n",
    "    \n",
    "    # Initialize f\n",
    "    f = g.copy().flatten()\n",
    "    \n",
    "    # Run optimization\n",
    "    for i in range(max_iter):\n",
    "        f_old = f.copy()\n",
    "        # Compute gradient and update f\n",
    "        grad = 2 * (D.T @ (A @ f - g)) + 2 * alpha * (gamma @ D @ f)\n",
    "        f -= np.linalg.norm(grad) * grad / np.linalg.norm(D @ grad)**2\n",
    "        # Project f onto [0, 1]\n",
    "        f[f < 0] = 0\n",
    "        f[f > 1] = 1\n",
    "        # Check stopping criterion\n",
    "        if np.linalg.norm(f - f_old) < eps:\n",
    "            break\n",
    "    \n",
    "    return f.reshape(g.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Iterative Deblurring with Anisotropic Derivative Filter\n",
    "\n",
    "To deblur the image iteratively with this method, we can follow these steps:\n",
    "\n",
    "1. Take an initial blurry image $f_0$ and set $i=0$.\n",
    "2. Compute the diffusivity $\\gamma(f_i)$.\n",
    "3. Compute the denoised image $f_{i+1}$ using the anisotropic derivative filter with the current value of $\\gamma(f_i)$.\n",
    "4. Increase $i$ by 1 and repeat from step 2 until convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def iterative_deblurring(g, A, alpha, T, max_iter=100, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Perform iterative deblurring with anisotropic derivative filter using the Perona-Malik diffusivity function.\n",
    "    \n",
    "    Args:\n",
    "        g (ndarray): the observed blurry image\n",
    "        A (ndarray): the blurring operator\n",
    "        alpha (float): the regularization strength\n",
    "        T (float): the threshold for the Perona-Malik function\n",
    "        max_iter (int): the maximum number of iterations (default: 100)\n",
    "        eps (float): the stopping criterion (default: 1e-5)\n",
    "        \n",
    "    Returns:\n",
    "        f (ndarray): the deblurred image\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    f = g.copy()\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    for i in range(max_iter):\n",
    "        # Compute diffusivity\n",
    "        gradient_norm = np.sqrt((Dx @ f.flatten())**2 + (Dy @ f.flatten())**2)\n",
    "        gamma = np.exp(-gradient_norm / T)\n",
    "        gamma = spdiags(gamma, 0, len(gradient_norm), len(gradient_norm)).todense()\n",
    "        \n",
    "        # Compute denoised image\n",
    "        f_old = f.copy()\n",
    "        f = spsolve(A.T @ A + alpha * D.T @ gamma @ D, A.T @ g.flatten())\n",
    "        f = f.reshape(g.shape)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(f - f_old) < eps:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
