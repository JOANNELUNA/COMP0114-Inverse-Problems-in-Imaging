{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOMP0114 Inverse Problems in Imaging. Coursework 2\n",
    "### Student ID: 18145399\n",
    "## Week 1 \n",
    "### 1. Convolution and deconvolution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Read a gray colormap image from the given URL and convert it to a float, normalise and display it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![solution](1a.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  Write a function that takes in an image $f$ and outputs the blurred image $Af$ with convolution mapping.\n",
    "$$g = Af_{true} + n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage.filters as filters\n",
    "\n",
    "def apply_convolution(f, sigma, theta):\n",
    "    # Apply Gaussian filter\n",
    "    g = filters.gaussian_filter(f, sigma)\n",
    "    \n",
    "    # Add noise to blurred image\n",
    "    w, h = g.shape\n",
    "    noise = np.random.randn(w, h)\n",
    "    g = g + theta * noise\n",
    "    \n",
    "    return g  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Deconvolve using normal equations, i.e. find $f_α$ as the solution to\n",
    "$$(A^TA + αI)f_α = A^Tg$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as splinalg\n",
    "\n",
    "def ATA_operator(f, sigma, alpha):\n",
    "    # Apply A^T A + alpha*I operator to f\n",
    "    Af = filters.gaussian_filter(f, sigma) \n",
    "    ATAf = filters.gaussian_filter(Af, sigma)\n",
    "    return ATAf + alpha * f\n",
    "\n",
    "def ATA(f, sigma, alpha):\n",
    "    # Compute A.TA(f) using Gaussian convolution\n",
    "    Af = filters.gaussian_filter(f, sigma)\n",
    "    Af = Af.reshape(-1, 1)\n",
    "    ATAf = filters.gaussian_filter(Af.reshape(f.shape), sigma)\n",
    "    ATAf = (ATAf + alpha * f).reshape(-1)\n",
    "    return ATAf\n",
    "\n",
    "def deconvolve_normal_equations(g, sigma, alpha):\n",
    "    # Set up linear operator for ATA\n",
    "    M, N = g.shape\n",
    "    A = sparse.linalg.LinearOperator((M*N, M*N), matvec=lambda x: np.ravel(ATA_operator(x.reshape(g.shape), sigma, alpha)))\n",
    "\n",
    "    # Compute ATg\n",
    "    ATg = np.ravel(g)\n",
    "\n",
    "    # Solve linear system using GMRES\n",
    "    f_alpha, info = splinalg.gmres(A, ATg)\n",
    "\n",
    "    return f_alpha.reshape((M, N))\n",
    "\n",
    "def normal_info(g, sigma, alpha):\n",
    "    # Set up linear operator for ATA\n",
    "    M, N = g.shape\n",
    "    A = sparse.linalg.LinearOperator((M*N, M*N), matvec=lambda x: np.ravel(ATA_operator(x.reshape(g.shape), sigma, alpha)))\n",
    "\n",
    "    # Compute ATg\n",
    "    ATg = np.ravel(g)\n",
    "\n",
    "    # Solve linear system using GMRES\n",
    "    f_alpha, info = splinalg.gmres(A, ATg)\n",
    "\n",
    "    return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Deconvolve by solving the augmented equations.\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "A\\\\\n",
    "\\sqrt{\\alpha}I\n",
    "\\end{pmatrix}f \n",
    "= \n",
    "\\begin{pmatrix}\n",
    "g\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_f(f):\n",
    "    # Implementation of the augmented matrix multiplication\n",
    "    y = filters.gaussian_filter(f, sigma)\n",
    "    z = filters.gaussian_filter(y, sigma)\n",
    "    M_f = np.vstack([np.ravel(z), np.sqrt(alpha)*np.ravel(f)])\n",
    "    return M_f\n",
    "\n",
    "def MT_b(b):\n",
    "    # Implementation of the transposed augmented matrix multiplication\n",
    "    global g\n",
    "    M, N = g.shape\n",
    "    g_vec = b[:M*N]\n",
    "    f_vec = b[M*N:]\n",
    "    g = np.reshape(g_vec, (M, N))\n",
    "    y = filters.gaussian_filter(g, sigma)\n",
    "    z = filters.gaussian_filter(y, sigma)\n",
    "    MT_b = np.ravel(z) + np.sqrt(alpha)*np.ravel(f_vec)\n",
    "    return MT_b\n",
    "\n",
    "def solve_augmented_equations(g, sigma, alpha):\n",
    "    # Define linear operator for lsqr\n",
    "    M, N = g.shape\n",
    "    size = M*N\n",
    "    A = sparse.linalg.LinearOperator((2*size, size), matvec=M_f, rmatvec=MT_b)\n",
    "\n",
    "    # Concatenate g with a zero vector\n",
    "    b = np.vstack([np.reshape(g,(size,1)), np.zeros((size, 1))])\n",
    "    \n",
    "    # Solve linear system using lsqr\n",
    "    f_lsqr= splinalg.lsqr(A, b)[0]\n",
    "\n",
    "    return f_lsqr[:g.size].reshape(g.shape)\n",
    "\n",
    "def augmented_info(g, sigma, alpha):\n",
    "    # Define linear operator for lsqr\n",
    "    M, N = g.shape\n",
    "    size = M*N\n",
    "    A = sparse.linalg.LinearOperator((2*size, size), matvec=M_f, rmatvec=MT_b)\n",
    "\n",
    "    # Concatenate g with a zero vector\n",
    "    b = np.vstack([np.reshape(g,(size,1)), np.zeros((size, 1))])\n",
    "    \n",
    "    # Solve linear system using lsqr\n",
    "    info = splinalg.lsqr(A, b)[1]\n",
    "\n",
    "    return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance to the one used in c.), in terms of number of iterations required to achieve convergence:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method c: Converged in 0 iterations (0.03956246376037598 seconds) \n",
    "\n",
    "Method d: Converged in 2 iterations (0.7001798152923584 seconds)\n",
    "\n",
    "![solution](1e.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "#### The choice of the value of alpha\n",
    "When the value of the regularization parameter alpha was set to 0.01, the deblurred image produced by Method c was unclear. However, when alpha was set to 1, the deblurred image became much clearer.\n",
    "\n",
    "It is possible that alpha=0.01 was too small, and the regularization term had little effect in suppressing the noise in the observed image. As a result, the noise dominated the solution, leading to an unclear deblurred image. On the other hand, alpha=1 may have been a better choice, as it struck a good balance between noise suppression and image fidelity, resulting in a clearer deblurred image.\n",
    "\n",
    "#### The choice of Deconvolution method \n",
    "Method c is faster and requires fewer iterations to converge than Method d.\n",
    "\n",
    "Method c uses the normal equations, which can be solved using a Krylov solver such as PCG or GMRES. The normal equations can be derived from the optimality conditions for the Tikhonov regularization problem, and their solution provides the optimal solution to the deblurring problem. Since the normal equations involve a symmetric positive definite matrix, a Krylov solver can efficiently solve the linear system without requiring an explicit matrix representation of the convolution operator.\n",
    "\n",
    "On the other hand, Method d uses an augmented equation approach, which involves solving a linear least squares problem using a solver such as lsqr. This method may require more iterations to converge because the least squares problem may not have a closed-form solution and the iterative solver may need to compute many iterations to approximate the solution. In addition, the implementation of the augmented equation method requires more memory to store the augmented system matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2:\n",
    "### 2. Choose a regularisation parameter α\n",
    "i)  Discrepency Principle\n",
    "\n",
    "The Discrepancy Principle method was used to find the optimal values of α for the solutions obtained using both the normal equation and augmented equation methods. The optimal value of α was found to be `1e-5` for the normal equation method and `1e-2` for the augmented equation method. The initial guess for the minimum value of α was set to `1e-5` for the normal equation method and `1e-2` for the augmented equation method, respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) L-Curve\n",
    "\n",
    "The L-curve method was used to determine the optimal value of α for the normal equation method, and it was found to be `5.34e-1`. However, this method cannot be used for the augmented equation method.\n",
    "\n",
    "![solution](lcurve.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "#### Difference in value obtained \n",
    "\n",
    "It's possible that the optimal alpha found by the L-curve method is different from the optimal alpha found by the Discrepancy Principle method because they use different criteria to determine the optimal alpha.\n",
    "\n",
    "The L-curve method tries to find the point on the L-curve that balances the trade-off between the residual norm and the regularization norm. This point represents the optimal alpha value that provides a good balance between overfitting and underfitting.\n",
    "\n",
    "On the other hand, the Discrepancy Principle method tries to find the alpha value that provides a solution that is consistent with the noise level in the data. The optimal alpha value is the one that satisfies the principle that the residual norm is approximately equal to the noise level.\n",
    "\n",
    "It's possible that the optimal alpha values found by these two methods are different because they are optimizing for different criteria. The L-curve method is optimizing for a trade-off between the residual and regularization norms, while the Discrepancy Principle method is optimizing for a solution that is consistent with the noise level.\n",
    "\n",
    "\n",
    "#### Results using these optimal alpha values\n",
    "##### 1) Results using DP optimal alpha value for normal equation\n",
    "Method c: Converged in 0 iterations (114.29729723930359 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.046361446380615234 seconds)\n",
    "\n",
    "![solution](result_dp_c.png)\n",
    "\n",
    "##### 2) Results using DP optimal alpha value for augmented equation\n",
    "Method c: Converged in 0 iterations (0.27139997482299805 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.04271340370178223 seconds)\n",
    "\n",
    "![solution](result_dp_d.png)\n",
    "\n",
    "##### 3) Results using L-curve optimal alpha value for normal equation\n",
    "Method c: Converged in 0 iterations (0.05376148223876953 seconds)\n",
    "\n",
    "Method d: Converged in 2 iterations (0.04639315605163574 seconds)\n",
    "\n",
    "![solution](result_l_c.png)\n",
    "\n",
    "Based on the fact that the optimal values of alpha found using the Discrepancy Principle method are the same as initial guess alphas, as well as the fact that the result of the deblurred plots are unclear when using the optimal alphas found by the DP method, it shows that Discrepancy Principle method is not a good choice to find the optimal value of α for the data being used.\n",
    "\n",
    "The Discrepancy Principle method can be useful for finding an approximate range of α values that may be optimal for the given data, but it may not always be able to pinpoint the exact optimal value. In my case, the optimal value of α may lie outside of the range searched by the DP method, or may require a more fine-tuned search to find.\n",
    "\n",
    "Therefore, it may be necessary to explore other methods, such as the L-curve method or other regularization methods, to find the optimal value of α for the given data. Additionally, it may be helpful to visually inspect the deblurred plots to determine if the result is acceptable or if further optimization is needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using a regularisation term based on the spatial derivative\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
